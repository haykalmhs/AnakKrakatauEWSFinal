{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def concat_json_files(folder_path):\n",
    "    combined_data = []\n",
    "\n",
    "    # Iterate through each file in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.json'):\n",
    "            with open(os.path.join(folder_path, filename), 'r') as file:\n",
    "                data = json.load(file)\n",
    "                combined_data.extend(data)\n",
    "\n",
    "    return combined_data\n",
    "\n",
    "# Path to the folder containing JSON files\n",
    "folder_path = '/run/media/viblab/Markov2/Haykal/AnakKrakatauEWS/data/raw/I06AU/I06AU_FK_RESULTS_300/NON_MERGE'\n",
    "\n",
    "# Concatenate the JSON data\n",
    "combined_data = concat_json_files(folder_path)\n",
    "\n",
    "# Write the combined data to a new JSON file\n",
    "with open('/run/media/viblab/Markov2/Haykal/AnakKrakatauEWS/data/raw/I06AU/I06AU_FD_150_W.json', 'w') as outfile:\n",
    "    json.dump(combined_data, outfile, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             File Path\n",
      "20   /run/media/viblab/Markov2/Haykal/AnakKrakatauE...\n",
      "60   /run/media/viblab/Markov2/Haykal/AnakKrakatauE...\n",
      "48   /run/media/viblab/Markov2/Haykal/AnakKrakatauE...\n",
      "288  /run/media/viblab/Markov2/Haykal/AnakKrakatauE...\n",
      "320  /run/media/viblab/Markov2/Haykal/AnakKrakatauE...\n",
      "..                                                 ...\n",
      "189  /run/media/viblab/Markov2/Haykal/AnakKrakatauE...\n",
      "172  /run/media/viblab/Markov2/Haykal/AnakKrakatauE...\n",
      "136  /run/media/viblab/Markov2/Haykal/AnakKrakatauE...\n",
      "201  /run/media/viblab/Markov2/Haykal/AnakKrakatauE...\n",
      "227  /run/media/viblab/Markov2/Haykal/AnakKrakatauE...\n",
      "\n",
      "[435 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def list_files(directory, extension):\n",
    "    files = []\n",
    "    # Walk through the directory\n",
    "    for root, dirs, files_in_dir in os.walk(directory):\n",
    "        for file in files_in_dir:\n",
    "            if file.endswith(extension):\n",
    "                full_path = os.path.join(root, file)\n",
    "                files.append(full_path)\n",
    "    return files\n",
    "\n",
    "# Directory containing the files\n",
    "directory = '/run/media/viblab/Markov2/Haykal/AnakKrakatauEWS/data/raw/I06AU/I06AU_FK_RESULTS_300'\n",
    "\n",
    "# List all '.fk_results.dat' files\n",
    "file_paths = list_files(directory, '.fk_results.dat')\n",
    "\n",
    "# Create a DataFrame from the file paths\n",
    "df = pd.DataFrame(file_paths, columns=['File Path'])\n",
    "\n",
    "# Sort the DataFrame alphabetically\n",
    "sorted_df = df.sort_values(by='File Path')\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "print(sorted_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df.to_csv('/run/media/viblab/Markov2/Haykal/AnakKrakatauEWS/notebook/FK_Database.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging completed. The output is saved as 'merged_output.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the first CSV file\n",
    "df1 = pd.read_csv('/run/media/viblab/Markov2/Haykal/AnakKrakatauEWS/data/features/I06H1_TEST_Dataset.csv')\n",
    "\n",
    "# Remove the 'cluster' column\n",
    "df1 = df1.drop(columns=['cluster'])\n",
    "\n",
    "# Load the second CSV file\n",
    "df2 = pd.read_csv('/run/media/viblab/Markov2/Haykal/AnakKrakatauEWS/data/Test_Segments_Timestamp.csv')\n",
    "\n",
    "# Merge the two dataframes on 'segment_id'\n",
    "merged_df = pd.merge(df1, df2, on='segment_id')\n",
    "\n",
    "# Save the merged dataframe to a new CSV file\n",
    "merged_df.to_csv('/run/media/viblab/Markov2/Haykal/AnakKrakatauEWS/data/features/I06H1_TEST_Dataset.csv', index=False)\n",
    "\n",
    "print(\"Merging completed. The output is saved as 'merged_output.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "data = pd.read_csv('/run/media/viblab/Markov2/Haykal/AnakKrakatauEWS/data/timeseries/Test_Train_Segments.csv')\n",
    "\n",
    "# Sort the DataFrame based on the 'Starttime' column\n",
    "data['Start_Time'] = pd.to_datetime(data['Start_Time'])  # Convert 'Starttime' to datetime if it's not already\n",
    "data.sort_values(by='Start_Time', inplace=True)\n",
    "\n",
    "# Save the sorted data to a new CSV file\n",
    "data.to_csv('/run/media/viblab/Markov2/Haykal/AnakKrakatauEWS/data/timeseries/Test_Train_Segments.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the main CSV file\n",
    "main_df = pd.read_csv('/run/media/viblab/Markov2/Haykal/AnakKrakatauEWS/data/features/TEST_Dataset.csv')\n",
    "\n",
    "# Load the second CSV file containing start and end times\n",
    "time_df = pd.read_csv('/run/media/viblab/Markov2/Haykal/AnakKrakatauEWS/data/Test.csv')\n",
    "\n",
    "# Convert the 'Start_Time' and 'End_Time' columns to datetime in the second DataFrame\n",
    "time_df['Start_Time'] = pd.to_datetime(time_df['Start_Time'])\n",
    "time_df['End_Time'] = pd.to_datetime(time_df['End_Time'])\n",
    "\n",
    "# Calculate the midpoint for each time span in the second DataFrame\n",
    "time_df['Timestamp'] = time_df['Start_Time'] + (time_df['End_Time'] - time_df['Start_Time']) / 2\n",
    "\n",
    "# Merge the midpoint data into the main DataFrame based on 'segment_id'\n",
    "merged_df = pd.merge(main_df, time_df[['segment_id', 'Timestamp']], on='segment_id', how='left')\n",
    "\n",
    "# Now 'merged_df' contains your main data along with the 'Midpoint' column\n",
    "print(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('/run/media/viblab/Markov2/Haykal/AnakKrakatauEWS/data/features/TEST_Dataset.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Datetime     I06H1     I06H2     I06H3     I06H4  \\\n",
      "3313200 2018-07-01 03:00:09.350000  0.656045  0.368027  0.421031  0.755403   \n",
      "3313201 2018-07-01 03:00:09.400000  0.611502  0.327639  0.364624  0.898304   \n",
      "3313202 2018-07-01 03:00:09.450000  0.497063  0.250486  0.453283  0.990035   \n",
      "3313203 2018-07-01 03:00:09.500000  0.279066  0.262781  0.664350  1.000000   \n",
      "3313204 2018-07-01 03:00:09.550000  0.053899  0.441433  0.791544  0.965469   \n",
      "...                            ...       ...       ...       ...       ...   \n",
      "24495   2019-08-10 21:10:06.949999  0.503063  0.055187  0.698695  0.843839   \n",
      "24496   2019-08-10 21:10:06.999999  0.578968  0.149821  0.521312  0.568478   \n",
      "24497   2019-08-10 21:10:07.049999  0.640236  0.236989  0.371320  0.371196   \n",
      "24498   2019-08-10 21:10:07.099999  0.706414  0.281897  0.348975  0.436399   \n",
      "24499   2019-08-10 21:10:07.149999  0.780988  0.308713  0.379025  0.589302   \n",
      "\n",
      "            I06H6     I06H7     I06H8  \n",
      "3313200  0.443750  0.361311  0.437246  \n",
      "3313201  0.468499  0.412302  0.515852  \n",
      "3313202  0.549191  0.441629  0.552015  \n",
      "3313203  0.644214  0.382978  0.550972  \n",
      "3313204  0.684081  0.301800  0.533076  \n",
      "...           ...       ...       ...  \n",
      "24495    0.422726  0.675126  0.457788  \n",
      "24496    0.570237  0.552105  0.520101  \n",
      "24497    0.688275  0.625382  0.579439  \n",
      "24498    0.725244  0.695917  0.602948  \n",
      "24499    0.633913  0.599765  0.605706  \n",
      "\n",
      "[22156800 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "def process_file(file_path):\n",
    "    # Read each file and sort\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "    return df.sort_values(by='Datetime')\n",
    "\n",
    "def read_csvs_from_folder(folder):\n",
    "    files = [os.path.join(folder, file) for file in os.listdir(folder) if file.endswith('.csv')]\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        # Use parallel processing to read and sort files\n",
    "        dataframes = list(executor.map(process_file, files))\n",
    "    return dataframes\n",
    "\n",
    "# Paths to your folders\n",
    "folder1 = '/run/media/viblab/Markov2/Haykal/AnakKrakatauEWS/data/test'\n",
    "folder2 = '/run/media/viblab/Markov2/Haykal/AnakKrakatauEWS/data/train'\n",
    "\n",
    "# Read and combine all CSV files from both folders using parallel processing\n",
    "df1 = read_csvs_from_folder(folder1)\n",
    "df2 = read_csvs_from_folder(folder2)\n",
    "\n",
    "# Concatenate all DataFrames and sort\n",
    "combined_df = pd.concat(df1 + df2, ignore_index=True).sort_values(by='Datetime')\n",
    "\n",
    "# Now 'combined_df' contains all your data, sorted by 'Datetime'\n",
    "print(combined_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Path to the folder containing the CSV files\n",
    "folder_path = '/run/media/viblab/Markov2/Haykal/AnakKrakatauEWS/data/raw/I06AU/I06AU_CSV_NEW'  # Replace with the path to your folder\n",
    "\n",
    "# Get a list of all CSV files in the folder\n",
    "csv_files = glob.glob(f\"{folder_path}/*.csv\")\n",
    "\n",
    "# Load each CSV file and append it to the list\n",
    "all_dataframes = []\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file, index_col=0, parse_dates=True)\n",
    "    all_dataframes.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "combined_df = pd.concat(all_dataframes)\n",
    "\n",
    "# Sort the combined DataFrame by the index (date)\n",
    "combined_df.sort_index(inplace=True)\n",
    "\n",
    "# Save the combined and sorted DataFrame to a new CSV file\n",
    "output_path = '/run/media/viblab/Markov2/Haykal/AnakKrakatauEWS/data/raw/I06AU/I06AU_WAVEFORM_ARRAY.csv'  # Replace with your desired output path\n",
    "combined_df.to_csv(output_path)\n",
    "\n",
    "print(f\"Combined CSV saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv('/run/media/viblab/Markov2/Haykal/AnakKrakatauEWS/data/TRAIN_TEST_Dataset.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/run/media/viblab/Markov2/Haykal/AnakKrakatauEWS/data/TRAIN_TEST_TS_Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (17725440, 8)\n",
      "Testing data shape: (4431360, 8)\n"
     ]
    }
   ],
   "source": [
    "# Sort the DataFrame by the datetime column\n",
    "df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "df.sort_values(by='Datetime', inplace=True)\n",
    "\n",
    "# Define the percentage split for training and testing data\n",
    "train_ratio = 0.8  # You can adjust this ratio as needed\n",
    "\n",
    "# Calculate the split index\n",
    "split_index = int(train_ratio * len(df))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data = df[:split_index]\n",
    "test_data = df[split_index:]\n",
    "\n",
    "# Display the shapes of the training and testing sets\n",
    "print(\"Training data shape:\", train_data.shape)\n",
    "print(\"Testing data shape:\", test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the two CSV files into DataFrames\n",
    "data1 = pd.read_csv('/run/media/viblab/Markov2/Haykal/AnakKrakatauEWS/data/Test_Segments_Timestamp.csv')\n",
    "data2 = pd.read_csv('data2.csv')\n",
    "\n",
    "# Merge data1 and data2 based on the 'segment_id' column\n",
    "merged_data = pd.merge(data1, data2[['segment_id', 'Timestamp']], on='segment_id', how='left')\n",
    "\n",
    "# Save the merged data to a new CSV file\n",
    "merged_data.to_csv('merged_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Notice Number  Volcanic Cloud Height (M)  Maximum Duration (Seconds)  \\\n",
      "1    2023KRA065                     1157.0                       232.0   \n",
      "2    2023KRA064                     2157.0                       182.0   \n",
      "3    2023KRA063                     1657.0                        59.0   \n",
      "4    2023KRA062                      657.0                        73.0   \n",
      "5    2023KRA061                     1657.0                       182.0   \n",
      "\n",
      "                  Starttime                   Endtime  \n",
      "1 2023-09-10 05:36:00+00:00 2023-09-10 05:39:52+00:00  \n",
      "2 2023-07-20 01:52:00+00:00 2023-07-20 01:55:02+00:00  \n",
      "3 2023-07-20 01:51:00+00:00 2023-07-20 01:51:59+00:00  \n",
      "4 2023-07-20 01:43:00+00:00 2023-07-20 01:44:13+00:00  \n",
      "5 2023-06-19 01:22:00+00:00 2023-06-19 01:25:02+00:00  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the VONA_Filtered CSV file\n",
    "vona_filtered_path = '/run/media/viblab/Markov2/Haykal/AnakKrakatauEWS/data/timeseries/VONA_Filtered.csv'  # Replace with your file path\n",
    "vona_filtered_df = pd.read_csv(vona_filtered_path)\n",
    "\n",
    "# Convert 'Issued' to datetime and rename the column to 'Starttime'\n",
    "vona_filtered_df['Starttime'] = pd.to_datetime(vona_filtered_df['Issued'])\n",
    "\n",
    "# Drop the original 'Issued' column\n",
    "vona_filtered_df.drop('Issued', axis=1, inplace=True)\n",
    "\n",
    "# Filter out rows where 'Maximum Duration (Seconds)' is missing\n",
    "vona_filtered_df = vona_filtered_df[vona_filtered_df['Maximum Duration (Seconds)'].notna()]\n",
    "\n",
    "# Calculate 'Endtime' by adding 'Maximum Duration (Seconds)' to 'Starttime'\n",
    "vona_filtered_df['Endtime'] = vona_filtered_df['Starttime'] + pd.to_timedelta(vona_filtered_df['Maximum Duration (Seconds)'], unit='s')\n",
    "\n",
    "# Display the modified DataFrame\n",
    "print(vona_filtered_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid comparison between dtype=datetime64[ns] and Timestamp",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/pandas/core/arrays/datetimelike.py:582\u001b[0m, in \u001b[0;36mDatetimeLikeArrayMixin._validate_comparison_value\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 582\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_compatible_with(other)\n\u001b[1;32m    583\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, IncompatibleFrequency) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    584\u001b[0m     \u001b[39m# e.g. tzawareness mismatch\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/pandas/core/arrays/datetimes.py:461\u001b[0m, in \u001b[0;36mDatetimeArray._check_compatible_with\u001b[0;34m(self, other, setitem)\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_assert_tzawareness_compat(other)\n\u001b[1;32m    462\u001b[0m \u001b[39mif\u001b[39;00m setitem:\n\u001b[1;32m    463\u001b[0m     \u001b[39m# Stricter check for setitem vs comparison methods\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/pandas/core/arrays/datetimes.py:694\u001b[0m, in \u001b[0;36mDatetimeArray._assert_tzawareness_compat\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[39mif\u001b[39;00m other_tz \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 694\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    695\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCannot compare tz-naive and tz-aware datetime-like objects.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    696\u001b[0m         )\n\u001b[1;32m    697\u001b[0m \u001b[39melif\u001b[39;00m other_tz \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot compare tz-naive and tz-aware datetime-like objects.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mInvalidComparison\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/pandas/core/arrays/datetimelike.py:1054\u001b[0m, in \u001b[0;36mDatetimeLikeArrayMixin._cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1054\u001b[0m     other \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_comparison_value(other)\n\u001b[1;32m   1055\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidComparison:\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/pandas/core/arrays/datetimelike.py:585\u001b[0m, in \u001b[0;36mDatetimeLikeArrayMixin._validate_comparison_value\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    583\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, IncompatibleFrequency) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    584\u001b[0m         \u001b[39m# e.g. tzawareness mismatch\u001b[39;00m\n\u001b[0;32m--> 585\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidComparison(other) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m    587\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_list_like(other):\n",
      "\u001b[0;31mInvalidComparison\u001b[0m: 2023-09-10 05:39:52+00:00",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/run/media/viblab/Markov2/Haykal/AnakKrakatauEWS/notebook/File_Concater.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bviblab-pcspeech/run/media/viblab/Markov2/Haykal/AnakKrakatauEWS/notebook/File_Concater.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     event_end \u001b[39m=\u001b[39m event[\u001b[39m'\u001b[39m\u001b[39mEndtime\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bviblab-pcspeech/run/media/viblab/Markov2/Haykal/AnakKrakatauEWS/notebook/File_Concater.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mif\u001b[39;00m pd\u001b[39m.\u001b[39mnotna(event_start) \u001b[39mand\u001b[39;00m pd\u001b[39m.\u001b[39mnotna(event_end):\n\u001b[0;32m---> <a href='vscode-notebook-cell://tunnel%2Bviblab-pcspeech/run/media/viblab/Markov2/Haykal/AnakKrakatauEWS/notebook/File_Concater.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m         mask \u001b[39m=\u001b[39m ((test_train_segments_df[\u001b[39m'\u001b[39;49m\u001b[39mStarttime\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m<\u001b[39;49m event_end) \u001b[39m&\u001b[39m (test_train_segments_df[\u001b[39m'\u001b[39m\u001b[39mEndtime\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m>\u001b[39m event_start))\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bviblab-pcspeech/run/media/viblab/Markov2/Haykal/AnakKrakatauEWS/notebook/File_Concater.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m         test_train_segments_df\u001b[39m.\u001b[39mloc[mask, \u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mEvent\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bviblab-pcspeech/run/media/viblab/Markov2/Haykal/AnakKrakatauEWS/notebook/File_Concater.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Display the result\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/pandas/core/ops/common.py:72\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[1;32m     70\u001b[0m other \u001b[39m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 72\u001b[0m \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39;49m, other)\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/pandas/core/arraylike.py:50\u001b[0m, in \u001b[0;36mOpsMixin.__lt__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39m@unpack_zerodim_and_defer\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m__lt__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__lt__\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[0;32m---> 50\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cmp_method(other, operator\u001b[39m.\u001b[39;49mlt)\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/pandas/core/series.py:6243\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6240\u001b[0m rvalues \u001b[39m=\u001b[39m extract_array(other, extract_numpy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, extract_range\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   6242\u001b[0m \u001b[39mwith\u001b[39;00m np\u001b[39m.\u001b[39merrstate(\u001b[39mall\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 6243\u001b[0m     res_values \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39;49mcomparison_op(lvalues, rvalues, op)\n\u001b[1;32m   6245\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_construct_result(res_values, name\u001b[39m=\u001b[39mres_name)\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/pandas/core/ops/array_ops.py:273\u001b[0m, in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    265\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mLengths must match to compare\u001b[39m\u001b[39m\"\u001b[39m, lvalues\u001b[39m.\u001b[39mshape, rvalues\u001b[39m.\u001b[39mshape\n\u001b[1;32m    266\u001b[0m         )\n\u001b[1;32m    268\u001b[0m \u001b[39mif\u001b[39;00m should_extension_dispatch(lvalues, rvalues) \u001b[39mor\u001b[39;00m (\n\u001b[1;32m    269\u001b[0m     (\u001b[39misinstance\u001b[39m(rvalues, (Timedelta, BaseOffset, Timestamp)) \u001b[39mor\u001b[39;00m right \u001b[39mis\u001b[39;00m NaT)\n\u001b[1;32m    270\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_object_dtype(lvalues\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    271\u001b[0m ):\n\u001b[1;32m    272\u001b[0m     \u001b[39m# Call the method on lvalues\u001b[39;00m\n\u001b[0;32m--> 273\u001b[0m     res_values \u001b[39m=\u001b[39m op(lvalues, rvalues)\n\u001b[1;32m    275\u001b[0m \u001b[39melif\u001b[39;00m is_scalar(rvalues) \u001b[39mand\u001b[39;00m isna(rvalues):  \u001b[39m# TODO: but not pd.NA?\u001b[39;00m\n\u001b[1;32m    276\u001b[0m     \u001b[39m# numpy does not like comparisons vs None\u001b[39;00m\n\u001b[1;32m    277\u001b[0m     \u001b[39mif\u001b[39;00m op \u001b[39mis\u001b[39;00m operator\u001b[39m.\u001b[39mne:\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/pandas/core/ops/common.py:72\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[1;32m     70\u001b[0m other \u001b[39m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 72\u001b[0m \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39;49m, other)\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/pandas/core/arraylike.py:50\u001b[0m, in \u001b[0;36mOpsMixin.__lt__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39m@unpack_zerodim_and_defer\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m__lt__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__lt__\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[0;32m---> 50\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cmp_method(other, operator\u001b[39m.\u001b[39;49mlt)\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/pandas/core/arrays/datetimelike.py:1056\u001b[0m, in \u001b[0;36mDatetimeLikeArrayMixin._cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   1054\u001b[0m     other \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_comparison_value(other)\n\u001b[1;32m   1055\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidComparison:\n\u001b[0;32m-> 1056\u001b[0m     \u001b[39mreturn\u001b[39;00m invalid_comparison(\u001b[39mself\u001b[39;49m, other, op)\n\u001b[1;32m   1058\u001b[0m dtype \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(other, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m   1059\u001b[0m \u001b[39mif\u001b[39;00m is_object_dtype(dtype):\n\u001b[1;32m   1060\u001b[0m     \u001b[39m# We have to use comp_method_OBJECT_ARRAY instead of numpy\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m     \u001b[39m#  comparison otherwise it would fail to raise when\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m     \u001b[39m#  comparing tz-aware and tz-naive\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/pandas/core/ops/invalid.py:36\u001b[0m, in \u001b[0;36minvalid_comparison\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     typ \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(right)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[0;32m---> 36\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid comparison between dtype=\u001b[39m\u001b[39m{\u001b[39;00mleft\u001b[39m.\u001b[39mdtype\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00mtyp\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[39mreturn\u001b[39;00m res_values\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid comparison between dtype=datetime64[ns] and Timestamp"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "test_train_segments_df = pd.read_csv('/run/media/viblab/Markov2/Haykal/AnakKrakatauEWS/data/timeseries/Test_Train_Segments.csv')\n",
    "vona_filtered_df = pd.read_csv('/run/media/viblab/Markov2/Haykal/AnakKrakatauEWS/data/timeseries/VONA_Filtered.csv')  # This is the preprocessed file\n",
    "\n",
    "# Convert 'Starttime' and 'Endtime' to datetime in both dataframes\n",
    "test_train_segments_df['Starttime'] = pd.to_datetime(test_train_segments_df['Starttime'], errors='coerce')\n",
    "test_train_segments_df['Endtime'] = pd.to_datetime(test_train_segments_df['Endtime'], errors='coerce')\n",
    "vona_filtered_df['Starttime'] = pd.to_datetime(vona_filtered_df['Starttime'], errors='coerce')\n",
    "vona_filtered_df['Endtime'] = pd.to_datetime(vona_filtered_df['Endtime'], errors='coerce')\n",
    "\n",
    "# Initialize a label column\n",
    "test_train_segments_df['label'] = 'No Event'\n",
    "\n",
    "# Label each segment based on overlap with events\n",
    "for _, event in vona_filtered_df.iterrows():\n",
    "    event_start = event['Starttime']\n",
    "    event_end = event['Endtime']\n",
    "    if pd.notna(event_start) and pd.notna(event_end):\n",
    "        mask = ((test_train_segments_df['Starttime'] < event_end) & (test_train_segments_df['Endtime'] > event_start))\n",
    "        test_train_segments_df.loc[mask, 'label'] = 'Event'\n",
    "\n",
    "# Display the result\n",
    "test_train_segments_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>I06H1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-07-01 03:00:09.350</td>\n",
       "      <td>0.656045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-07-01 03:00:09.400</td>\n",
       "      <td>0.611502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-07-01 03:00:09.450</td>\n",
       "      <td>0.497063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-07-01 03:00:09.500</td>\n",
       "      <td>0.279066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-07-01 03:00:09.550</td>\n",
       "      <td>0.053899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Datetime     I06H1\n",
       "0  2018-07-01 03:00:09.350  0.656045\n",
       "1  2018-07-01 03:00:09.400  0.611502\n",
       "2  2018-07-01 03:00:09.450  0.497063\n",
       "3  2018-07-01 03:00:09.500  0.279066\n",
       "4  2018-07-01 03:00:09.550  0.053899"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_train_test = pd.read_csv('/run/media/viblab/Markov2/Haykal/AnakKrakatauEWS/data/timeseries/TRAIN_TEST_TS_Dataset.csv')\n",
    "df_train_test = df_train_test.drop(['I06H2','I06H3','I06H4','I06H6','I06H7', 'I06H8'], axis=1)\n",
    "df_train_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22156800, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df_train_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Define the folder path that contains your CSV files\n",
    "folder_path = '/run/media/viblab/Markov2/Haykal/AnakKrakatauEWS/data/processed2_stft/Features_Real_test'\n",
    "\n",
    "# Get a list of all CSV files in the folder\n",
    "csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "\n",
    "# List to hold dataframes\n",
    "dfs = []\n",
    "\n",
    "# Loop over the list of csv files\n",
    "for file in csv_files:\n",
    "    # Read the csv file\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    # Parse the start_time column as datetime and set as index\n",
    "    df['start_time'] = pd.to_datetime(df['start_time'])\n",
    "    df = df.set_index('start_time')\n",
    "    \n",
    "    # Append the dataframe to the list\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all dataframes into one\n",
    "combined_df = pd.concat(dfs)\n",
    "\n",
    "# Sort by index (start_time)\n",
    "combined_df = combined_df.sort_index()\n",
    "\n",
    "# Optionally, you can save the combined dataframe to a new csv file\n",
    "combined_df.to_csv('/run/media/viblab/Markov2/Haykal/AnakKrakatauEWS/data/processed2_stft/Real_test_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2018-06-24 22:50:00+00:00' '2018-06-24 22:51:15+00:00'\n",
      " '2018-06-25 20:30:17.200000+00:00' ... '2019-01-31 18:16:46.100000+00:00'\n",
      " '2019-01-31 19:25:31.100000+00:00' '2019-01-31 19:26:46.100000+00:00']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('/run/media/viblab/Markov2/Haykal/AnakKrakatauEWS/data/processed2_stft/hello.csv')\n",
    "array = np.array(df.index)\n",
    "print(array)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Timestamp('2019-02-01 13:25:00+0000', tz='UTC')\n",
      " Timestamp('2019-02-01 13:26:15+0000', tz='UTC')\n",
      " Timestamp('2019-02-01 14:45:00+0000', tz='UTC')\n",
      " Timestamp('2019-02-01 14:46:15+0000', tz='UTC')\n",
      " Timestamp('2019-02-02 00:10:19.900000+0000', tz='UTC')\n",
      " Timestamp('2019-02-02 00:11:34.900000+0000', tz='UTC')\n",
      " Timestamp('2019-02-02 00:12:49.900000+0000', tz='UTC')\n",
      " Timestamp('2019-02-02 00:14:04.900000+0000', tz='UTC')\n",
      " Timestamp('2019-02-02 04:20:19.900000+0000', tz='UTC')\n",
      " Timestamp('2019-02-02 04:21:34.900000+0000', tz='UTC')\n",
      " Timestamp('2019-02-02 04:22:49.900000+0000', tz='UTC')\n",
      " Timestamp('2019-02-02 04:24:04.900000+0000', tz='UTC')\n",
      " Timestamp('2019-02-02 05:12:49.900000+0000', tz='UTC')\n",
      " Timestamp('2019-02-02 05:14:04.900000+0000', tz='UTC')\n",
      " Timestamp('2019-02-02 09:22:49.900000+0000', tz='UTC')\n",
      " Timestamp('2019-02-02 09:24:04.900000+0000', tz='UTC')\n",
      " Timestamp('2019-02-02 11:25:19.900000+0000', tz='UTC')\n",
      " Timestamp('2019-02-02 11:26:34.900000+0000', tz='UTC')\n",
      " Timestamp('2019-02-02 11:35:19.900000+0000', tz='UTC')\n",
      " Timestamp('2019-02-02 11:36:34.900000+0000', tz='UTC')\n",
      " Timestamp('2019-02-02 11:37:49.900000+0000', tz='UTC')\n",
      " Timestamp('2019-02-02 11:39:04.900000+0000', tz='UTC')\n",
      " Timestamp('2019-02-02 11:52:49.900000+0000', tz='UTC')\n",
      " Timestamp('2019-02-02 11:54:04.900000+0000', tz='UTC')\n",
      " Timestamp('2019-02-02 12:27:49.900000+0000', tz='UTC')\n",
      " Timestamp('2019-02-02 12:29:04.900000+0000', tz='UTC')\n",
      " Timestamp('2019-02-02 15:27:49.900000+0000', tz='UTC')\n",
      " Timestamp('2019-02-02 15:29:04.900000+0000', tz='UTC')\n",
      " Timestamp('2019-02-02 15:42:49.900000+0000', tz='UTC')\n",
      " Timestamp('2019-02-02 15:44:04.900000+0000', tz='UTC')\n",
      " Timestamp('2019-02-02 16:45:19.900000+0000', tz='UTC')\n",
      " Timestamp('2019-02-02 16:46:34.900000+0000', tz='UTC')\n",
      " Timestamp('2019-02-02 16:47:49.900000+0000', tz='UTC')\n",
      " Timestamp('2019-02-02 16:49:04.900000+0000', tz='UTC')\n",
      " Timestamp('2019-02-02 16:50:19.900000+0000', tz='UTC')\n",
      " Timestamp('2019-02-02 16:51:34.900000+0000', tz='UTC')\n",
      " Timestamp('2019-02-03 01:10:39.150000+0000', tz='UTC')\n",
      " Timestamp('2019-02-03 01:11:54.150000+0000', tz='UTC')\n",
      " Timestamp('2019-02-03 12:00:39.150000+0000', tz='UTC')\n",
      " Timestamp('2019-02-03 12:01:54.150000+0000', tz='UTC')\n",
      " Timestamp('2019-02-03 12:13:09.150000+0000', tz='UTC')\n",
      " Timestamp('2019-02-03 12:14:24.150000+0000', tz='UTC')\n",
      " Timestamp('2019-02-03 13:10:39.150000+0000', tz='UTC')\n",
      " Timestamp('2019-02-03 13:11:54.150000+0000', tz='UTC')\n",
      " Timestamp('2019-02-03 19:58:09.150000+0000', tz='UTC')\n",
      " Timestamp('2019-02-03 19:59:24.150000+0000', tz='UTC')\n",
      " Timestamp('2019-02-03 20:25:39.150000+0000', tz='UTC')\n",
      " Timestamp('2019-02-03 20:26:54.150000+0000', tz='UTC')\n",
      " Timestamp('2019-02-03 20:28:09.150000+0000', tz='UTC')\n",
      " Timestamp('2019-02-03 20:29:24.150000+0000', tz='UTC')\n",
      " Timestamp('2019-02-03 20:30:39.150000+0000', tz='UTC')\n",
      " Timestamp('2019-02-03 20:31:54.150000+0000', tz='UTC')\n",
      " Timestamp('2019-02-03 20:33:09.150000+0000', tz='UTC')\n",
      " Timestamp('2019-02-03 20:34:24.150000+0000', tz='UTC')\n",
      " Timestamp('2019-02-03 21:50:39.150000+0000', tz='UTC')\n",
      " Timestamp('2019-02-03 21:51:54.150000+0000', tz='UTC')\n",
      " Timestamp('2019-02-03 23:45:39.150000+0000', tz='UTC')\n",
      " Timestamp('2019-02-03 23:46:54.150000+0000', tz='UTC')\n",
      " Timestamp('2019-02-03 23:48:09.150000+0000', tz='UTC')\n",
      " Timestamp('2019-02-03 23:49:24.150000+0000', tz='UTC')\n",
      " Timestamp('2019-02-08 18:15:17.600000+0000', tz='UTC')\n",
      " Timestamp('2019-02-08 18:16:32.600000+0000', tz='UTC')\n",
      " Timestamp('2019-02-08 18:17:47.600000+0000', tz='UTC')\n",
      " Timestamp('2019-02-08 18:19:02.600000+0000', tz='UTC')\n",
      " Timestamp('2019-02-08 19:50:17.600000+0000', tz='UTC')\n",
      " Timestamp('2019-02-08 19:51:32.600000+0000', tz='UTC')\n",
      " Timestamp('2019-02-08 20:47:47.600000+0000', tz='UTC')\n",
      " Timestamp('2019-02-08 20:49:02.600000+0000', tz='UTC')\n",
      " Timestamp('2019-02-09 15:10:37.700000+0000', tz='UTC')\n",
      " Timestamp('2019-02-09 15:11:52.700000+0000', tz='UTC')\n",
      " Timestamp('2019-02-09 15:18:07.700000+0000', tz='UTC')\n",
      " Timestamp('2019-02-09 15:19:22.700000+0000', tz='UTC')\n",
      " Timestamp('2019-02-09 17:40:37.700000+0000', tz='UTC')\n",
      " Timestamp('2019-02-09 17:41:52.700000+0000', tz='UTC')\n",
      " Timestamp('2019-02-09 18:55:37.700000+0000', tz='UTC')\n",
      " Timestamp('2019-02-09 18:56:52.700000+0000', tz='UTC')\n",
      " Timestamp('2019-02-09 18:58:07.700000+0000', tz='UTC')\n",
      " Timestamp('2019-02-09 18:59:22.700000+0000', tz='UTC')\n",
      " Timestamp('2019-02-09 19:00:37.700000+0000', tz='UTC')\n",
      " Timestamp('2019-02-09 19:01:52.700000+0000', tz='UTC')\n",
      " Timestamp('2019-02-09 19:03:07.700000+0000', tz='UTC')\n",
      " Timestamp('2019-02-09 19:04:22.700000+0000', tz='UTC')\n",
      " Timestamp('2019-02-09 19:05:37.700000+0000', tz='UTC')\n",
      " Timestamp('2019-02-09 19:06:52.700000+0000', tz='UTC')\n",
      " Timestamp('2019-02-09 19:08:07.700000+0000', tz='UTC')\n",
      " Timestamp('2019-02-09 19:09:22.700000+0000', tz='UTC')\n",
      " Timestamp('2019-02-09 19:15:37.700000+0000', tz='UTC')\n",
      " Timestamp('2019-02-09 19:16:52.700000+0000', tz='UTC')\n",
      " Timestamp('2019-02-09 23:10:37.700000+0000', tz='UTC')\n",
      " Timestamp('2019-02-09 23:11:52.700000+0000', tz='UTC')\n",
      " Timestamp('2019-02-09 23:23:07.700000+0000', tz='UTC')\n",
      " Timestamp('2019-02-09 23:24:22.700000+0000', tz='UTC')\n",
      " Timestamp('2019-02-10 00:02:31.200000+0000', tz='UTC')\n",
      " Timestamp('2019-02-10 00:03:46.200000+0000', tz='UTC')\n",
      " Timestamp('2019-02-10 07:27:31.200000+0000', tz='UTC')\n",
      " Timestamp('2019-02-10 07:28:46.200000+0000', tz='UTC')\n",
      " Timestamp('2019-02-10 14:15:01.200000+0000', tz='UTC')\n",
      " Timestamp('2019-02-10 14:16:16.200000+0000', tz='UTC')\n",
      " Timestamp('2019-02-10 15:02:31.200000+0000', tz='UTC')\n",
      " Timestamp('2019-02-10 15:03:46.200000+0000', tz='UTC')\n",
      " Timestamp('2019-02-10 15:55:01.200000+0000', tz='UTC')\n",
      " Timestamp('2019-02-10 15:56:16.200000+0000', tz='UTC')\n",
      " Timestamp('2019-02-10 15:57:31.200000+0000', tz='UTC')\n",
      " Timestamp('2019-02-10 15:58:46.200000+0000', tz='UTC')\n",
      " Timestamp('2019-02-10 16:00:01.200000+0000', tz='UTC')\n",
      " Timestamp('2019-02-10 16:01:16.200000+0000', tz='UTC')\n",
      " Timestamp('2019-02-11 09:45:18.400000+0000', tz='UTC')\n",
      " Timestamp('2019-02-11 09:46:33.400000+0000', tz='UTC')\n",
      " Timestamp('2019-02-11 09:55:18.400000+0000', tz='UTC')\n",
      " Timestamp('2019-02-11 09:56:33.400000+0000', tz='UTC')\n",
      " Timestamp('2019-02-11 10:17:48.400000+0000', tz='UTC')\n",
      " Timestamp('2019-02-11 10:19:03.400000+0000', tz='UTC')\n",
      " Timestamp('2019-02-11 10:25:18.400000+0000', tz='UTC')\n",
      " Timestamp('2019-02-11 10:26:33.400000+0000', tz='UTC')\n",
      " Timestamp('2019-02-11 10:40:18.400000+0000', tz='UTC')\n",
      " Timestamp('2019-02-11 10:41:33.400000+0000', tz='UTC')\n",
      " Timestamp('2019-02-11 10:42:48.400000+0000', tz='UTC')\n",
      " Timestamp('2019-02-11 10:44:03.400000+0000', tz='UTC')\n",
      " Timestamp('2019-02-11 11:30:18.400000+0000', tz='UTC')\n",
      " Timestamp('2019-02-11 11:31:33.400000+0000', tz='UTC')\n",
      " Timestamp('2019-02-11 12:00:18.400000+0000', tz='UTC')\n",
      " Timestamp('2019-02-11 12:01:33.400000+0000', tz='UTC')\n",
      " Timestamp('2019-02-11 12:02:48.400000+0000', tz='UTC')\n",
      " Timestamp('2019-02-11 12:04:03.400000+0000', tz='UTC')\n",
      " Timestamp('2019-02-13 18:29:00+0000', tz='UTC')\n",
      " Timestamp('2019-02-13 18:30:15+0000', tz='UTC')\n",
      " Timestamp('2019-02-15 17:58:01.650000+0000', tz='UTC')\n",
      " Timestamp('2019-02-15 17:59:16.650000+0000', tz='UTC')\n",
      " Timestamp('2019-02-15 22:50:31.650000+0000', tz='UTC')\n",
      " Timestamp('2019-02-15 22:51:46.650000+0000', tz='UTC')\n",
      " Timestamp('2019-02-16 18:10:00+0000', tz='UTC')\n",
      " Timestamp('2019-02-16 18:11:15+0000', tz='UTC')\n",
      " Timestamp('2019-02-17 22:47:49.350000+0000', tz='UTC')\n",
      " Timestamp('2019-02-17 22:49:04.350000+0000', tz='UTC')\n",
      " Timestamp('2019-02-18 21:20:36.400000+0000', tz='UTC')\n",
      " Timestamp('2019-02-18 21:21:51.400000+0000', tz='UTC')\n",
      " Timestamp('2019-02-19 07:55:01.350000+0000', tz='UTC')\n",
      " Timestamp('2019-02-19 07:56:16.350000+0000', tz='UTC')\n",
      " Timestamp('2019-02-19 17:40:01.350000+0000', tz='UTC')\n",
      " Timestamp('2019-02-19 17:41:16.350000+0000', tz='UTC')\n",
      " Timestamp('2019-02-19 18:45:01.350000+0000', tz='UTC')\n",
      " Timestamp('2019-02-19 18:46:16.350000+0000', tz='UTC')\n",
      " Timestamp('2019-02-20 17:05:17.050000+0000', tz='UTC')\n",
      " Timestamp('2019-02-20 17:06:32.050000+0000', tz='UTC')\n",
      " Timestamp('2019-02-21 20:37:56.500000+0000', tz='UTC')\n",
      " Timestamp('2019-02-21 20:39:11.500000+0000', tz='UTC')\n",
      " Timestamp('2019-02-22 08:37:28.250000+0000', tz='UTC')\n",
      " Timestamp('2019-02-22 08:38:43.250000+0000', tz='UTC')\n",
      " Timestamp('2019-02-25 12:10:01.800000+0000', tz='UTC')\n",
      " Timestamp('2019-02-25 12:11:16.800000+0000', tz='UTC')\n",
      " Timestamp('2019-02-26 00:50:13.900000+0000', tz='UTC')\n",
      " Timestamp('2019-02-26 00:51:28.900000+0000', tz='UTC')\n",
      " Timestamp('2019-02-26 07:15:13.900000+0000', tz='UTC')\n",
      " Timestamp('2019-02-26 07:16:28.900000+0000', tz='UTC')\n",
      " Timestamp('2019-02-26 09:42:43.900000+0000', tz='UTC')\n",
      " Timestamp('2019-02-26 09:43:58.900000+0000', tz='UTC')\n",
      " Timestamp('2019-02-26 18:17:43.900000+0000', tz='UTC')\n",
      " Timestamp('2019-02-26 18:18:58.900000+0000', tz='UTC')\n",
      " Timestamp('2019-02-26 19:27:43.900000+0000', tz='UTC')\n",
      " Timestamp('2019-02-26 19:28:58.900000+0000', tz='UTC')\n",
      " Timestamp('2019-02-26 21:17:43.900000+0000', tz='UTC')\n",
      " Timestamp('2019-02-26 21:18:58.900000+0000', tz='UTC')\n",
      " Timestamp('2019-02-27 11:40:33.050000+0000', tz='UTC')\n",
      " Timestamp('2019-02-27 11:41:48.050000+0000', tz='UTC')\n",
      " Timestamp('2019-03-06 18:35:02.500000+0000', tz='UTC')\n",
      " Timestamp('2019-03-06 18:36:17.500000+0000', tz='UTC')\n",
      " Timestamp('2019-03-06 18:37:32.500000+0000', tz='UTC')\n",
      " Timestamp('2019-03-06 18:38:47.500000+0000', tz='UTC')\n",
      " Timestamp('2019-03-07 02:00:18.500000+0000', tz='UTC')\n",
      " Timestamp('2019-03-07 02:01:33.500000+0000', tz='UTC')\n",
      " Timestamp('2019-03-08 09:30:32.750000+0000', tz='UTC')\n",
      " Timestamp('2019-03-08 09:31:47.750000+0000', tz='UTC')\n",
      " Timestamp('2019-03-08 09:33:02.750000+0000', tz='UTC')\n",
      " Timestamp('2019-03-08 09:34:17.750000+0000', tz='UTC')\n",
      " Timestamp('2019-03-08 09:35:32.750000+0000', tz='UTC')\n",
      " Timestamp('2019-03-08 09:36:47.750000+0000', tz='UTC')\n",
      " Timestamp('2019-03-08 20:28:02.750000+0000', tz='UTC')\n",
      " Timestamp('2019-03-08 20:29:17.750000+0000', tz='UTC')\n",
      " Timestamp('2019-03-08 21:45:32.750000+0000', tz='UTC')\n",
      " Timestamp('2019-03-08 21:46:47.750000+0000', tz='UTC')\n",
      " Timestamp('2019-03-08 21:53:02.750000+0000', tz='UTC')\n",
      " Timestamp('2019-03-08 21:54:17.750000+0000', tz='UTC')\n",
      " Timestamp('2019-03-08 21:55:32.750000+0000', tz='UTC')\n",
      " Timestamp('2019-03-08 21:56:47.750000+0000', tz='UTC')\n",
      " Timestamp('2019-03-08 21:58:02.750000+0000', tz='UTC')\n",
      " Timestamp('2019-03-08 21:59:17.750000+0000', tz='UTC')\n",
      " Timestamp('2019-03-08 22:05:32.750000+0000', tz='UTC')\n",
      " Timestamp('2019-03-08 22:06:47.750000+0000', tz='UTC')\n",
      " Timestamp('2019-03-08 22:08:02.750000+0000', tz='UTC')\n",
      " Timestamp('2019-03-08 22:09:17.750000+0000', tz='UTC')\n",
      " Timestamp('2019-03-08 22:15:32.750000+0000', tz='UTC')\n",
      " Timestamp('2019-03-08 22:16:47.750000+0000', tz='UTC')\n",
      " Timestamp('2019-03-09 00:40:00+0000', tz='UTC')\n",
      " Timestamp('2019-03-09 00:41:15+0000', tz='UTC')\n",
      " Timestamp('2019-03-09 09:32:30+0000', tz='UTC')\n",
      " Timestamp('2019-03-09 09:33:45+0000', tz='UTC')\n",
      " Timestamp('2019-03-09 09:42:30+0000', tz='UTC')\n",
      " Timestamp('2019-03-09 09:43:45+0000', tz='UTC')\n",
      " Timestamp('2019-03-09 18:00:00+0000', tz='UTC')\n",
      " Timestamp('2019-03-09 18:01:15+0000', tz='UTC')\n",
      " Timestamp('2019-03-09 18:02:30+0000', tz='UTC')\n",
      " Timestamp('2019-03-09 18:03:45+0000', tz='UTC')\n",
      " Timestamp('2019-03-09 18:05:00+0000', tz='UTC')\n",
      " Timestamp('2019-03-09 18:06:15+0000', tz='UTC')\n",
      " Timestamp('2019-03-10 06:57:48.700000+0000', tz='UTC')\n",
      " Timestamp('2019-03-10 06:59:03.700000+0000', tz='UTC')\n",
      " Timestamp('2019-03-10 09:30:18.700000+0000', tz='UTC')\n",
      " Timestamp('2019-03-10 09:31:33.700000+0000', tz='UTC')\n",
      " Timestamp('2019-03-10 09:32:48.700000+0000', tz='UTC')\n",
      " Timestamp('2019-03-10 09:34:03.700000+0000', tz='UTC')\n",
      " Timestamp('2019-03-10 09:35:18.700000+0000', tz='UTC')\n",
      " Timestamp('2019-03-10 09:36:33.700000+0000', tz='UTC')\n",
      " Timestamp('2019-03-10 09:37:48.700000+0000', tz='UTC')\n",
      " Timestamp('2019-03-10 09:39:03.700000+0000', tz='UTC')\n",
      " Timestamp('2019-03-10 11:02:48.700000+0000', tz='UTC')\n",
      " Timestamp('2019-03-10 11:04:03.700000+0000', tz='UTC')\n",
      " Timestamp('2019-03-10 21:35:18.700000+0000', tz='UTC')\n",
      " Timestamp('2019-03-10 21:36:33.700000+0000', tz='UTC')\n",
      " Timestamp('2019-03-11 17:53:09.700000+0000', tz='UTC')\n",
      " Timestamp('2019-03-11 17:54:24.700000+0000', tz='UTC')\n",
      " Timestamp('2019-03-12 01:20:00.550000+0000', tz='UTC')\n",
      " Timestamp('2019-03-12 01:21:15.550000+0000', tz='UTC')\n",
      " Timestamp('2019-03-12 01:22:30.550000+0000', tz='UTC')\n",
      " Timestamp('2019-03-12 01:23:45.550000+0000', tz='UTC')\n",
      " Timestamp('2019-03-12 01:25:00.550000+0000', tz='UTC')\n",
      " Timestamp('2019-03-12 01:26:15.550000+0000', tz='UTC')\n",
      " Timestamp('2019-03-16 07:02:48.650000+0000', tz='UTC')\n",
      " Timestamp('2019-03-16 07:04:03.650000+0000', tz='UTC')\n",
      " Timestamp('2019-03-17 08:45:34.200000+0000', tz='UTC')\n",
      " Timestamp('2019-03-17 08:46:49.200000+0000', tz='UTC')\n",
      " Timestamp('2019-03-20 17:08:03.450000+0000', tz='UTC')\n",
      " Timestamp('2019-03-20 17:09:18.450000+0000', tz='UTC')\n",
      " Timestamp('2019-03-20 17:10:33.450000+0000', tz='UTC')\n",
      " Timestamp('2019-03-20 17:11:48.450000+0000', tz='UTC')\n",
      " Timestamp('2019-03-20 17:13:03.450000+0000', tz='UTC')\n",
      " Timestamp('2019-03-20 17:14:18.450000+0000', tz='UTC')\n",
      " Timestamp('2019-03-20 23:30:33.450000+0000', tz='UTC')\n",
      " Timestamp('2019-03-20 23:31:48.450000+0000', tz='UTC')\n",
      " Timestamp('2019-03-20 23:33:03.450000+0000', tz='UTC')\n",
      " Timestamp('2019-03-20 23:34:18.450000+0000', tz='UTC')\n",
      " Timestamp('2019-03-20 23:35:33.450000+0000', tz='UTC')\n",
      " Timestamp('2019-03-20 23:36:48.450000+0000', tz='UTC')\n",
      " Timestamp('2019-03-20 23:38:03.450000+0000', tz='UTC')\n",
      " Timestamp('2019-03-20 23:39:18.450000+0000', tz='UTC')\n",
      " Timestamp('2019-03-20 23:40:33.450000+0000', tz='UTC')\n",
      " Timestamp('2019-03-20 23:41:48.450000+0000', tz='UTC')\n",
      " Timestamp('2019-03-20 23:48:03.450000+0000', tz='UTC')\n",
      " Timestamp('2019-03-20 23:49:18.450000+0000', tz='UTC')\n",
      " Timestamp('2019-03-20 23:50:33.450000+0000', tz='UTC')\n",
      " Timestamp('2019-03-20 23:51:48.450000+0000', tz='UTC')\n",
      " Timestamp('2019-03-21 18:30:00+0000', tz='UTC')\n",
      " Timestamp('2019-03-21 18:31:15+0000', tz='UTC')\n",
      " Timestamp('2019-03-21 21:32:30+0000', tz='UTC')\n",
      " Timestamp('2019-03-21 21:33:45+0000', tz='UTC')\n",
      " Timestamp('2019-03-22 12:15:18.650000+0000', tz='UTC')\n",
      " Timestamp('2019-03-22 12:16:33.650000+0000', tz='UTC')\n",
      " Timestamp('2019-03-24 01:52:30+0000', tz='UTC')\n",
      " Timestamp('2019-03-24 01:53:45+0000', tz='UTC')\n",
      " Timestamp('2019-03-24 19:05:00+0000', tz='UTC')\n",
      " Timestamp('2019-03-24 19:06:15+0000', tz='UTC')\n",
      " Timestamp('2019-03-24 19:07:30+0000', tz='UTC')\n",
      " Timestamp('2019-03-24 19:08:45+0000', tz='UTC')\n",
      " Timestamp('2019-03-25 00:05:16.350000+0000', tz='UTC')\n",
      " Timestamp('2019-03-25 00:06:31.350000+0000', tz='UTC')\n",
      " Timestamp('2019-03-25 14:27:46.350000+0000', tz='UTC')\n",
      " Timestamp('2019-03-25 14:29:01.350000+0000', tz='UTC')\n",
      " Timestamp('2019-03-25 18:12:46.350000+0000', tz='UTC')\n",
      " Timestamp('2019-03-25 18:14:01.350000+0000', tz='UTC')\n",
      " Timestamp('2019-03-25 18:20:16.350000+0000', tz='UTC')\n",
      " Timestamp('2019-03-25 18:21:31.350000+0000', tz='UTC')\n",
      " Timestamp('2019-03-25 20:47:46.350000+0000', tz='UTC')\n",
      " Timestamp('2019-03-25 20:49:01.350000+0000', tz='UTC')\n",
      " Timestamp('2019-03-25 21:55:16.350000+0000', tz='UTC')\n",
      " Timestamp('2019-03-25 21:56:31.350000+0000', tz='UTC')\n",
      " Timestamp('2019-03-25 22:15:16.350000+0000', tz='UTC')\n",
      " Timestamp('2019-03-25 22:16:31.350000+0000', tz='UTC')\n",
      " Timestamp('2019-03-25 22:17:46.350000+0000', tz='UTC')\n",
      " Timestamp('2019-03-25 22:19:01.350000+0000', tz='UTC')\n",
      " Timestamp('2019-03-25 22:27:46.350000+0000', tz='UTC')\n",
      " Timestamp('2019-03-25 22:29:01.350000+0000', tz='UTC')\n",
      " Timestamp('2019-03-25 23:17:46.350000+0000', tz='UTC')\n",
      " Timestamp('2019-03-25 23:19:01.350000+0000', tz='UTC')\n",
      " Timestamp('2019-03-26 00:03:03.950000+0000', tz='UTC')\n",
      " Timestamp('2019-03-26 00:04:18.950000+0000', tz='UTC')\n",
      " Timestamp('2019-03-26 01:28:03.950000+0000', tz='UTC')\n",
      " Timestamp('2019-03-26 01:29:18.950000+0000', tz='UTC')\n",
      " Timestamp('2019-03-26 01:45:33.950000+0000', tz='UTC')\n",
      " Timestamp('2019-03-26 01:46:48.950000+0000', tz='UTC')\n",
      " Timestamp('2019-03-26 02:53:03.950000+0000', tz='UTC')\n",
      " Timestamp('2019-03-26 02:54:18.950000+0000', tz='UTC')\n",
      " Timestamp('2019-03-26 02:55:33.950000+0000', tz='UTC')\n",
      " Timestamp('2019-03-26 02:56:48.950000+0000', tz='UTC')\n",
      " Timestamp('2019-03-26 02:58:03.950000+0000', tz='UTC')\n",
      " Timestamp('2019-03-26 02:59:18.950000+0000', tz='UTC')\n",
      " Timestamp('2019-03-26 03:00:33.950000+0000', tz='UTC')\n",
      " Timestamp('2019-03-26 03:01:48.950000+0000', tz='UTC')\n",
      " Timestamp('2019-03-26 03:35:33.950000+0000', tz='UTC')\n",
      " Timestamp('2019-03-26 03:36:48.950000+0000', tz='UTC')\n",
      " Timestamp('2019-03-26 03:45:33.950000+0000', tz='UTC')\n",
      " Timestamp('2019-03-26 03:46:48.950000+0000', tz='UTC')\n",
      " Timestamp('2019-03-26 03:48:03.950000+0000', tz='UTC')\n",
      " Timestamp('2019-03-26 03:49:18.950000+0000', tz='UTC')\n",
      " Timestamp('2019-03-26 03:50:33.950000+0000', tz='UTC')\n",
      " Timestamp('2019-03-26 03:51:48.950000+0000', tz='UTC')\n",
      " Timestamp('2019-03-26 03:53:03.950000+0000', tz='UTC')\n",
      " Timestamp('2019-03-26 03:54:18.950000+0000', tz='UTC')\n",
      " Timestamp('2019-03-26 04:00:33.950000+0000', tz='UTC')\n",
      " Timestamp('2019-03-26 04:01:48.950000+0000', tz='UTC')\n",
      " Timestamp('2019-03-26 13:40:33.950000+0000', tz='UTC')\n",
      " Timestamp('2019-03-26 13:41:48.950000+0000', tz='UTC')\n",
      " Timestamp('2019-03-26 14:35:33.950000+0000', tz='UTC')\n",
      " Timestamp('2019-03-26 14:36:48.950000+0000', tz='UTC')\n",
      " Timestamp('2019-03-26 15:08:03.950000+0000', tz='UTC')\n",
      " Timestamp('2019-03-26 15:09:18.950000+0000', tz='UTC')\n",
      " Timestamp('2019-03-26 15:10:33.950000+0000', tz='UTC')\n",
      " Timestamp('2019-03-26 15:11:48.950000+0000', tz='UTC')\n",
      " Timestamp('2019-03-26 15:13:03.950000+0000', tz='UTC')\n",
      " Timestamp('2019-03-26 15:14:18.950000+0000', tz='UTC')\n",
      " Timestamp('2019-03-26 15:15:33.950000+0000', tz='UTC')\n",
      " Timestamp('2019-03-26 15:16:48.950000+0000', tz='UTC')\n",
      " Timestamp('2019-03-26 16:08:03.950000+0000', tz='UTC')\n",
      " Timestamp('2019-03-26 16:09:18.950000+0000', tz='UTC')\n",
      " Timestamp('2019-03-27 02:05:00.850000+0000', tz='UTC')\n",
      " Timestamp('2019-03-27 02:06:15.850000+0000', tz='UTC')\n",
      " Timestamp('2019-03-29 10:53:05+0000', tz='UTC')\n",
      " Timestamp('2019-03-29 10:54:20+0000', tz='UTC')\n",
      " Timestamp('2019-04-02 19:50:01.200000+0000', tz='UTC')\n",
      " Timestamp('2019-04-02 19:51:16.200000+0000', tz='UTC')\n",
      " Timestamp('2019-04-05 12:52:34.100000+0000', tz='UTC')\n",
      " Timestamp('2019-04-05 12:53:49.100000+0000', tz='UTC')\n",
      " Timestamp('2019-04-12 21:47:47.550000+0000', tz='UTC')\n",
      " Timestamp('2019-04-12 21:49:02.550000+0000', tz='UTC')\n",
      " Timestamp('2019-04-20 13:37:31.900000+0000', tz='UTC')\n",
      " Timestamp('2019-04-20 13:38:46.900000+0000', tz='UTC')\n",
      " Timestamp('2019-04-20 22:45:01.900000+0000', tz='UTC')\n",
      " Timestamp('2019-04-20 22:46:16.900000+0000', tz='UTC')\n",
      " Timestamp('2019-04-20 22:47:31.900000+0000', tz='UTC')\n",
      " Timestamp('2019-04-20 22:48:46.900000+0000', tz='UTC')\n",
      " Timestamp('2019-04-22 00:12:59.250000+0000', tz='UTC')\n",
      " Timestamp('2019-04-22 00:14:14.250000+0000', tz='UTC')\n",
      " Timestamp('2019-05-08 19:55:00+0000', tz='UTC')\n",
      " Timestamp('2019-05-08 19:56:15+0000', tz='UTC')\n",
      " Timestamp('2019-05-09 00:42:46+0000', tz='UTC')\n",
      " Timestamp('2019-05-09 00:44:01+0000', tz='UTC')\n",
      " Timestamp('2019-05-09 03:20:16+0000', tz='UTC')\n",
      " Timestamp('2019-05-09 03:21:31+0000', tz='UTC')\n",
      " Timestamp('2019-05-12 17:25:17.750000+0000', tz='UTC')\n",
      " Timestamp('2019-05-12 17:26:32.750000+0000', tz='UTC')\n",
      " Timestamp('2019-05-12 21:37:47.750000+0000', tz='UTC')\n",
      " Timestamp('2019-05-12 21:39:02.750000+0000', tz='UTC')\n",
      " Timestamp('2019-06-04 16:32:30.800000+0000', tz='UTC')\n",
      " Timestamp('2019-06-04 16:33:45.800000+0000', tz='UTC')\n",
      " Timestamp('2019-06-16 04:37:30.250000+0000', tz='UTC')\n",
      " Timestamp('2019-06-16 04:38:45.250000+0000', tz='UTC')\n",
      " Timestamp('2019-06-20 21:00:15.750000+0000', tz='UTC')\n",
      " Timestamp('2019-06-20 21:01:30.750000+0000', tz='UTC')\n",
      " Timestamp('2019-07-08 08:37:45.400000+0000', tz='UTC')\n",
      " Timestamp('2019-07-08 08:39:00.400000+0000', tz='UTC')\n",
      " Timestamp('2019-07-19 05:00:00.800000+0000', tz='UTC')\n",
      " Timestamp('2019-07-19 05:01:15.800000+0000', tz='UTC')\n",
      " Timestamp('2019-07-29 11:52:46.400000+0000', tz='UTC')\n",
      " Timestamp('2019-07-29 11:54:01.400000+0000', tz='UTC')\n",
      " Timestamp('2019-08-03 04:05:02.400000+0000', tz='UTC')\n",
      " Timestamp('2019-08-03 04:06:17.400000+0000', tz='UTC')\n",
      " Timestamp('2019-08-04 11:00:13+0000', tz='UTC')\n",
      " Timestamp('2019-08-04 11:01:28+0000', tz='UTC')\n",
      " Timestamp('2019-08-05 16:50:24.700000+0000', tz='UTC')\n",
      " Timestamp('2019-08-05 16:51:39.700000+0000', tz='UTC')\n",
      " Timestamp('2019-08-05 16:52:54.700000+0000', tz='UTC')\n",
      " Timestamp('2019-08-05 16:54:09.700000+0000', tz='UTC')\n",
      " Timestamp('2019-08-05 16:55:24.700000+0000', tz='UTC')\n",
      " Timestamp('2019-08-05 16:56:39.700000+0000', tz='UTC')\n",
      " Timestamp('2019-08-05 23:32:54.700000+0000', tz='UTC')\n",
      " Timestamp('2019-08-05 23:34:09.700000+0000', tz='UTC')\n",
      " Timestamp('2019-08-05 23:35:24.700000+0000', tz='UTC')\n",
      " Timestamp('2019-08-05 23:36:39.700000+0000', tz='UTC')\n",
      " Timestamp('2019-08-06 04:45:00+0000', tz='UTC')\n",
      " Timestamp('2019-08-06 04:46:15+0000', tz='UTC')\n",
      " Timestamp('2019-08-06 04:47:30+0000', tz='UTC')\n",
      " Timestamp('2019-08-06 04:48:45+0000', tz='UTC')\n",
      " Timestamp('2019-08-06 04:50:00+0000', tz='UTC')\n",
      " Timestamp('2019-08-06 04:51:15+0000', tz='UTC')\n",
      " Timestamp('2019-08-06 04:52:30+0000', tz='UTC')\n",
      " Timestamp('2019-08-06 04:53:45+0000', tz='UTC')\n",
      " Timestamp('2019-08-06 04:55:00+0000', tz='UTC')\n",
      " Timestamp('2019-08-06 04:56:15+0000', tz='UTC')\n",
      " Timestamp('2019-08-06 04:57:30+0000', tz='UTC')\n",
      " Timestamp('2019-08-06 04:58:45+0000', tz='UTC')\n",
      " Timestamp('2019-08-06 05:00:00+0000', tz='UTC')\n",
      " Timestamp('2019-08-06 05:01:15+0000', tz='UTC')\n",
      " Timestamp('2019-08-07 04:20:12.500000+0000', tz='UTC')\n",
      " Timestamp('2019-08-07 04:21:27.500000+0000', tz='UTC')\n",
      " Timestamp('2019-08-07 16:25:12.500000+0000', tz='UTC')\n",
      " Timestamp('2019-08-07 16:26:27.500000+0000', tz='UTC')\n",
      " Timestamp('2019-08-10 20:42:47.200000+0000', tz='UTC')\n",
      " Timestamp('2019-08-10 20:44:02.200000+0000', tz='UTC')\n",
      " Timestamp('2019-08-10 20:50:17.200000+0000', tz='UTC')\n",
      " Timestamp('2019-08-10 20:51:32.200000+0000', tz='UTC')\n",
      " Timestamp('2019-08-10 21:10:17.200000+0000', tz='UTC')\n",
      " Timestamp('2019-08-10 21:11:32.200000+0000', tz='UTC')\n",
      " Timestamp('2019-08-21 22:50:40+0000', tz='UTC')\n",
      " Timestamp('2019-08-21 22:51:55+0000', tz='UTC')\n",
      " Timestamp('2019-08-21 23:45:40+0000', tz='UTC')\n",
      " Timestamp('2019-08-21 23:46:55+0000', tz='UTC')\n",
      " Timestamp('2019-08-22 07:35:11.900000+0000', tz='UTC')\n",
      " Timestamp('2019-08-22 07:36:26.900000+0000', tz='UTC')\n",
      " Timestamp('2019-08-22 10:22:41.900000+0000', tz='UTC')\n",
      " Timestamp('2019-08-22 10:23:56.900000+0000', tz='UTC')\n",
      " Timestamp('2019-08-26 01:00:27.450000+0000', tz='UTC')\n",
      " Timestamp('2019-08-26 01:01:42.450000+0000', tz='UTC')\n",
      " Timestamp('2019-08-26 05:05:27.450000+0000', tz='UTC')\n",
      " Timestamp('2019-08-26 05:06:42.450000+0000', tz='UTC')\n",
      " Timestamp('2019-08-26 05:07:57.450000+0000', tz='UTC')\n",
      " Timestamp('2019-08-26 05:09:12.450000+0000', tz='UTC')\n",
      " Timestamp('2019-08-26 05:30:27.450000+0000', tz='UTC')\n",
      " Timestamp('2019-08-26 05:31:42.450000+0000', tz='UTC')\n",
      " Timestamp('2019-08-26 14:37:57.450000+0000', tz='UTC')\n",
      " Timestamp('2019-08-26 14:39:12.450000+0000', tz='UTC')\n",
      " Timestamp('2019-08-26 22:57:57.450000+0000', tz='UTC')\n",
      " Timestamp('2019-08-26 22:59:12.450000+0000', tz='UTC')\n",
      " Timestamp('2019-09-01 17:10:26.950000+0000', tz='UTC')\n",
      " Timestamp('2019-09-01 17:11:41.950000+0000', tz='UTC')]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/run/media/viblab/Markov2/Haykal/AnakKrakatauEWS/notebook/File_Concater.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bviblab-pcspeech/run/media/viblab/Markov2/Haykal/AnakKrakatauEWS/notebook/File_Concater.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(start_time_array)\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bviblab-pcspeech/run/media/viblab/Markov2/Haykal/AnakKrakatauEWS/notebook/File_Concater.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Save the numpy array as a .npy file\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://tunnel%2Bviblab-pcspeech/run/media/viblab/Markov2/Haykal/AnakKrakatauEWS/notebook/File_Concater.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m np\u001b[39m.\u001b[39;49msave(\u001b[39m'\u001b[39;49m\u001b[39m/run/media/viblab/Markov2/Haykal/AnakKrakatauEWS/data/processed2_stft/Test_Datetime_Array.npy\u001b[39;49m\u001b[39m'\u001b[39;49m, start_time_array)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('/run/media/viblab/Markov2/Haykal/AnakKrakatauEWS/data/processed2_stft/Real_test_data.csv')\n",
    "\n",
    "# Convert the \"start_time\" column to a numpy array of UTC datetime objects\n",
    "start_time_array = pd.to_datetime(df['start_time']).to_numpy()\n",
    "print(start_time_array)\n",
    "\n",
    "# Save the numpy array as a .npy file\n",
    "np.save('/run/media/viblab/Markov2/Haykal/AnakKrakatauEWS/data/processed2_stft/Test_Datetime_Array.npy', start_time_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DateTime</th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-06-24 22:50:00.000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-06-24 22:51:15.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-06-25 20:30:17.200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-06-25 20:31:32.200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-06-26 02:50:32.150</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 DateTime  Cluster\n",
       "0 2018-06-24 22:50:00.000        1\n",
       "1 2018-06-24 22:51:15.000        0\n",
       "2 2018-06-25 20:30:17.200        0\n",
       "3 2018-06-25 20:31:32.200        0\n",
       "4 2018-06-26 02:50:32.150        0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's load the necessary libraries and the provided data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the numpy array containing the datetime index\n",
    "datetime_index = np.load('/run/media/viblab/Markov2/Haykal/AnakKrakatauEWS/data/processed2_stft/datetimeindex2.npy')\n",
    "datetime_index = pd.to_datetime(datetime_index)  # Convert to pandas datetime index\n",
    "\n",
    "# Load the cluster labels\n",
    "y = np.loadtxt('/run/media/viblab/Markov2/Haykal/AnakKrakatauEWS/data/processed2_stft/Km-n3-ft_Rewritten.txt')\n",
    "\n",
    "# Combine the datetime index and the cluster labels into a DataFrame\n",
    "df = pd.DataFrame({'DateTime': datetime_index, 'Cluster': y.astype(int)})\n",
    "\n",
    "df.head()  # Show the first few rows of the DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/run/media/viblab/Markov2/Haykal/AnakKrakatauEWS/data/processed2_stft/clustered_datetime.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
